title: 对于SEO的一些总结
date: 2016-08-04 16:52:39
tags: [SEO,程序开发,总结]
---
## 前言

**SEO**是由英文**Search Engine Optimization**缩写而来， 中文意译为“搜索引擎优化”。SEO是指通过站内优化比如网站结构调整、网站内容建设、网站代码优化等及站外优化，比如网站站外推广、网站品牌建设等，使网站满足搜索引擎收录排名需求，在搜索引擎中提高关键词排名。（来自百度百科）

总得来说，就是提高在网页搜索引擎中的排名，这里排除直接付费竞价等方式提高排名的方式，仅讨论在网站代码优化上下功夫。

<!-- more -->

## 搜索引擎是如何工作的

1. 用户输入一个词
2. 查询分类（因为分类很重要，这是一个重要的信息检索的问题）
3. 扫描组织类似于反向索引的的文档集合匹配查询一组相关文件
4. 查询结果会按照相关排序返回给用户（本文就是讨论如何提高返回搜索结果的排序的问题）

但是上述只是过程的一半，早在你输入您的关键字之前，搜索引擎需要对全网进行收集以及整理。具体如下：

1. 一个网络爬虫爬行你的网站（和在你的网站上的每一个页面）
2. 索引的所有页面（意思是一个搜索引擎需要解析和储存在互联网上的每一个页面内容）
3. 组织网络爬虫收集和存储到的数据与索引对应的元数据，以提供极快的和相关的搜索结果（在这一步有两个有趣的问题——[文档分类](http://en.wikipedia.org/wiki/Document_classification)和如何建立一个[高性能搜索引擎索引](http://en.wikipedia.org/wiki/Index_%28search_engine%29)）

顺序返回结果取决于程度的相关性。通俗地说，相关性是一个在给定的（爬虫爬过的）页面中关键字的数量查询和页面域名的权威性。

相关性计算公式：相关性 = #查询关键字 * 域名的权威性

域名的权威性包含很多因素，但是它很大程度上跟“链接”有关。很明显，一个搜索引擎关注的是你在你的网页中说了些什么，但是他们更多关心的是其他人是怎么评论你的（换种说法就是其他人如何链接你的网页）。所以总结来说，域名的权威性是一个进来的链接和相关的评论的结合。

网页排名（PageRank）是域名权威性的另一个重要部分。最简单的网页排名方式就是“被链接的流行度”（原文是popularity）——就是每个链接你的网页数量就是你的“流行度”。一个很受欢迎的网站，就会获得更大的“流行度”得分。当然，这样的算法很容易作弊（即链接工厂，循环连接等），所以谷歌已经改变了网站权威性的影响因素——包括域名的多样性、链接你的网站的重要性等。

使用权威性过滤掉垃圾或恶意网站很有意义——很难伪造100或1000个链接去提高相关网站的搜索。使用锚文本链接回一个网站相关性的各种查询强烈的符号（这也解释了为什么运用关键字作为你的网站名字有助于那个关键字的你的网站排名，所有锚文本到您的网站将使用你的品牌或网站名字）。

## 确保你的网站能够被爬虫爬到

在引擎爬虫过来的时候有两个关键考虑要点：使你的网站（每个网页）可以被发现，和确保你的内容被正确指引。这些细节处理可以保证你的所有页面可以被机器人（爬虫）抓到，而且当机器人（爬虫）看到页面时候可以看到所有相关的内容。

确保页面渲染不启用JavaScript——但浏览器像爬虫一样会启用。

最酷的事情就是所以Ajax和JavaScript帮助你选择性地呈现事物和动态生成内容，但不利的一面是,对于没有JavaScript的人(或在这种情况下机器人)，你需要确保你（内容）的正确性。

运用JavaScript浏览你的网站,确保所有的链接和页面可访问性和呈现的内容（火狐Firefox开发者工具让你可以轻易地不启用JavaScript去浏览网页）。

为了让网络爬虫注册相关页面，你去需要确保他们（爬虫）可以看到所以内容。所以如果你悬哦加载一群动态文本使用Ajax,或JavaScript，那就创建一个不启用JavaScript的版本,将其显示相同的信息。

除了确保网络爬虫程序可以达到页面和看到你的内容，你也应该注意在你的网站内部页面如何构建链接和锚文本。锚文本是一个搜索引擎的算法的关键部分，所以如果你在互联网上不要有任何其他页面链接到站点上的页面（就像你刚刚开了你的博客，你友情链接了别人，但至今没有人链接回给你），之后你可以做的最好的事情就是确保你自己的内部使用相关关键词锚文本的链接。

**使用描述性的锚文本页面，甚至在你的自己的网站上**

通常这是有利于建立面包屑导航，它有助于锚文本和在你的网站上提供相关的内部链接和锚文本。

**限制页面上的链接数目**

关于这个有很多原因，但通常来说如果你的页面上有太多的链接，你的网站会被搜索引擎认为是垃圾网站。

**网络爬虫不能索引他不能看到的页面**

搜索结果归因于一个页面的内容。如果你使用了Ajax，那么请确保每个页面的内容都有其自己的URL链接。有时候你看到Ajax站点，用户可以交互和渲染页面而没有出现一个新的URL,从可用性经历看起来棒极了，但是如果你还不做一个允许爬虫访问所有内容没有Ajax版本,，它会阻碍你的搜索引擎排名。

**遵循URL最佳实践**

如果你使用新的URL集合方式，它可能会伤害到你的SEO排名。例如，加上一个“#”作为URL的一部分就像“http://www.yourdomain.com/p#product8” ——谷歌引擎对待上面这情况会认为这是一个锚而不是唯一的URL，因此如果是一个独特的页面的话可以考虑准备查询参数。

## 关键字有利于提高pagerank

这里列出关键字放的地方 ——不过要确保你的做好所有以上基础优化。

1. 把他们（关键字）放到URL中（甚至最好您的域名名称有你的关键字）
2. 页面的title
3. 有一个h1的HTML标签（你可以运用CSS改变h1默认的样式）
4. 把 alt 标签放入图片中（还有其他项目如视频等），使用描述性图片名字。

如果有疑问时查看的 [可访问性标准](http://www.w3.org/standards/webdesign/accessibility)  ——alt注释是给屏幕读者看的，也是给搜索引擎看的。

**页面有上下文相关文本内容**

如果你阅读相关TFIDF指南和LDA指南，你可以了解到一点关于分类你的网站的一些基础性算法。如果你理解了这些指南文件，那么机器人具有良好的重要性可读的文本内容页面应该是显而易见的。只要确保它没有看到不重复内容。

关于关键字顺序问题，所以把最相关的方面并对在页面的顶部。这就是为什么好的SEO规定页面“标题 | 站点名称”为标题，而不是相反。

但在最后提醒一点，太多的关键词(或关键字优化)是一个垃圾网站的标志（对于搜索引擎来说），因此选择合理的关键字。

## 避免重复的内容
谷歌（和其他搜索引擎一样）使用重复检测算法来检测你的网站（这方面详细可以参考： http://infolab.stanford.edu/~ullman/mmds/ch3.pdf）

避免复制网络上的重复的内容（除非你能补充很多内容进去让它（文章或者内容）看起来跟以前不一样，新闻的标题和片段用在我们的产品页面）。这个（方法）非常适用于你的网站里面的页面，内容重复可以混淆一个搜索引擎的页面的权威性(如果你只是剪切和粘贴别人的内容，当然它也可以惩罚你的网站排名)，然后你自己的页面之间会彼此竞争结果排名。

如果你必须重复内容,使用“rel = canonical”标志让搜索引擎知道哪个URL更加有权威性。但是如果你复制了人家的副本，那么就想一些策略来增加更多的文本和信息来区分你的页面，因为这样的重复内容是不可能获得不错的排名。


## 使用好的Meta标签

这些Meta小片段描述出现在搜索结果页面压低你的链接排名。这些Meta实际上对于SEO来说并不重要，但是meta对于你想更多的人去点击你的链接至关重要！

适当的Meta描述能让用户快速确定真的是他们正在寻找什么，这可以从众多搜索结果中大大提高点击率。

## 保持更新

谷歌爬虫喜欢爬经常更新（有质量内容）的网站。

常更新的网站通常排名都会比较靠前，这样确保你的网站的至少一部分需要被常规性的更新；企业博客是一个很好的方法（加上可以给你一个地方为你的用户添加上下文相关内容）。

如果你有博客，那就不要把整篇文章放在（首页）页面上。为什么？还有重复的在你的网站页面上，直到该页面进入存档。（所以如果你可以选择，那就只在首页显示摘录吧）

## 加快网站的加载速度

谷歌表示在他们的算法页面加载速度问题，所以一定要确保你已经调整您的网站，都服从最佳做法，以使事情迅速。

### 1.对网站页面进行静态化处理
将动态页面转化为实际存在的静态页面这种方法, 由于静态页面的存在, 少了动态解析过程, 所以提高了页面的访问速度和稳定性, 使得优化效果非常明显。目前 CMS系统实现 URL静态化的方法可以使用 MVC三层架构, 通过 Rewrite 技术实现了 URL伪静态。 URL Rewrite方式特点鲜明, 由于是服务器内部解析的地址, 所以内容是实时更新的, 也不存在文件管理和硬件问题, 维护比较方便。在服务器级 URLRewrite 重写技术并不影响页面的执行速度。如果可以实现自定义URL生成规则, 甚至包括后缀名, 这样将更能在 URL中突出 Keyword,提高网页的权重。

### 2.采用CSS+DIV布局网站
采用 CSS+DIV的网页在搜索引擎优化方面的优势要强于传统采用 Table 编写的网页。 对于以内容为主的 CMS系统来说采用 CSS+DIV的模式可以将文章的内容放到更加靠前的位置, 以便于搜索引擎蜘蛛更快地找到它所需的内容。而且从网页浏览速度上考虑, 采用 CSS+DIV重构的页面容量要比 Table 编码的页面文件容量小得多, 前者一般只有后者的 1/2 大小。 使用 DIV+CSS布局, 页面代码变得精简。 代码精简所带来的直接好处有两点: 一是提高搜索引擎蜘蛛的爬行效率,能在最短的时间内爬完整个页面, 这样对收录质量有一定好处; 二是由于能高效的爬行, 就会受到搜索引擎蜘蛛的喜欢, 这样对收录数量有一定好处。

### 3.避免SessionID的生成
不少电子商务网站都会对所有访客自动产生 Session ID, 这也十分不可取。因为搜索引擎蜘蛛每次来的时候都会得到一个不同的Session ID, 这样同一个页面就会产生多种 URL, 造成复制内容网页。如果需要的话, 应该是客户登录以后再产生 Session ID, 对未登录的一般访问完全没有必要产生一个 Session ID。

### 4.使用外部Javascript和CSS文件
不管是由 CMS系统生成的网站, 还是普通网站都常会犯的一个错误就是, 把 Java 和 CSS放在网页的最前面, 把真正的内容推到了很后面。 在实际应用中, 使用外部 Java 和 CSS文件可以提高页面的速度, 因为 Java 和 CSS文件都能在浏览器中产生缓存, 在没有增加 HTTP 请求次数的同时可以减少 HTML文档的大小。而内置在HTML文档中的 Java 和 CSS则会在每次请求中随 HTML文档重新下载, 这虽然减少了 HTTP 请求的次数, 却增加了 HTML 文档的大小。

### 5.建立导航
就是在每个话题的具体帖子下面出现一个与之内容相关的帖子导航。 一种方式是为文章建立多个关键词, 并在文章内容下面列出, 当用户点击这些关键词, 自动进入该关键词的搜索页面。第二种方式是在文章内容下面提供相关文章列表, 自定义规则、 显示规则, 譬如, 按哪个关键词、 是按相关度来展示还是按时间展示等, 在内容页中显示本类下的 TOP10、 推荐文章并建立一个随机内容区域, 用来展示本类下的文章。





